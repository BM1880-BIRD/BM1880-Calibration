
# Build docker 
```
docker build -t bmcalibration:2.1 -f docker/Dockerfile .

```

# Run docker
```
docker run -it bmcalibration:2.1

```

# Run calibration
python main.py model_name model_path [--memory_opt]
ex.
    python main.py custom ./CustomModel --memory_opt

# Input data layer
1. The example prototxt is using general_data_layer.py as data layer to read the calibration image and do some preprocess to the image.
2. The preprocess method must be match with the method you use when training your model.
3. You can also use caffe original data layer like lmdb.

# GPU support
1. Make sure nvidia-docker is installed.
2. Copy so files in the caffe_gpu directory to caffe.
3. Change 'FROM ubuntu:16.04' to 'FROM nvidia/cuda:8.0-cudnn7-devel-ubuntu16.04' in the docker/DockerFile and run
   'nvidia-docker build -t bmcalibration:2.0 -f docker/DockerFile .'
4. Run 'nvidia-docker run -v /workspace:/workspace -it bmcalibration:2.0'
5. Uncomment 'caffe.set_mode_gpu()' and 'caffe.set_device(0) in main.py and run it. You can use GPU to run calibration now!

# Note
1. All needed third-party package is listed in docker/DockerFile.
2. You can create a new data layer .py file which include your own image preprocess method and put it in the 'module' param of your prototxt.
3. '--iteration' parameter is merged into main.py. All other parameters are the same as 1.0.

# Test in int8 mode
1. Use int8_init api to init int8 caffe net. **Make sure you are using our modified caffe or you will not have int8_init.**
   ex.
	net = caffe.Net('deploy.prototxt', 'deploy.caffemodel')
	net.int8_init('deploy.prototxt', 'int8.caffemodel', 'calibration_table.pb2', '')
	out = net.forward()
2. The fourth parameter of int8_init is the layers that you want to test in int8 mode. For example if you specify 'conv1' that means only conv1 layer
   forwards in int8 mode and all other layers are still in fp32 mode. You can use ',' to append more than one layer. Empty string means all layers.
3. Becareful that the input will auto quantize when you use the default caffe input layer in the prototxt.
   ex.
	layer {
	    name: "data"
	    type: "Input"
	    top: "data"
	    input_param {
		shape {
		    dim: 1
		    dim: 3
		    dim: 224
		    dim: 224
	        }
	    }
	}
   Otherwise you should do quantize yourself.

